{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 48, 48) (35887,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the Dataset\n",
    "train_dir = 'Dataset/Train'\n",
    "test_dir = 'Dataset/Test'\n",
    "\n",
    "def get_data(dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for cat in os.listdir(dir):\n",
    "        for img_name in os.listdir(os.path.join(dir, cat)):\n",
    "            img = cv2.imread(os.path.join(dir,cat,img_name), cv2.IMREAD_GRAYSCALE)\n",
    "            images.append(img)\n",
    "            labels.append(cat)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "train_images, train_labels = get_data(train_dir)\n",
    "test_images, test_labels = get_data(test_dir)\n",
    "\n",
    "all_images = np.concatenate((train_images, test_images))\n",
    "all_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "print(all_images.shape, all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
      "happy       8989\n",
      "neutral     6198\n",
      "sad         6077\n",
      "fear        5121\n",
      "angry       4953\n",
      "surprise    4002\n",
      "disgust      547\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(all_labels))\n",
    "# describe the labels\n",
    "print(pd.Series(all_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert flattened images into a list of strings for saving\n",
    "# image_strings = [' '.join(map(str, img)) for img in all_images]\n",
    "\n",
    "# # Create a DataFrame with image data as a single column and labels\n",
    "# df = pd.DataFrame({'image_data': image_strings, 'label': all_labels})\n",
    "\n",
    "# # Save to CSV\n",
    "# df.to_csv('all_images_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to all_images_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the images and labels to a CSV file\n",
    "def save_images_labels_to_csv(images, labels, output_csv):\n",
    "    with open(output_csv, 'w') as f:\n",
    "        # Write the header for image pixels and the label\n",
    "        columns = ['label'] + [f'pixel_{i}' for i in range(images[0].size)] \n",
    "        f.write(','.join(columns) + '\\n')\n",
    "        \n",
    "        # Process and write each image's flattened data\n",
    "        for image, label in zip(images, labels):\n",
    "            # Flatten the image to a 1D array\n",
    "            flattened_image = image.flatten()\n",
    "            # Convert the array to a string format suitable for CSV\n",
    "            row = np.append(label, flattened_image)\n",
    "            row_str = ','.join(map(str, row))\n",
    "            # Write the row to the CSV\n",
    "            f.write(row_str + '\\n')\n",
    "    \n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "# Save all images and labels to a single CSV file\n",
    "save_images_labels_to_csv(all_images, all_labels, 'all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (28709, 48, 48)\n",
      "Test data shape: (7178, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Train data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the images from the csv\n",
    "image_size = 48  # Assuming 48x48 images\n",
    "\n",
    "# Load the data from the CSV\n",
    "def load_images_labels_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    labels = df['label'].values\n",
    "    # Drop the label column to retain only pixel data\n",
    "    image_data = df.drop(columns=['label']).values\n",
    "    # Reshape the image data back to original shape (48x48)\n",
    "    images = [np.array(image, dtype=np.uint8).reshape((image_size, image_size)) for image in image_data]\n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels\n",
    "images, labels = load_images_labels_from_csv('all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    return hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "\n",
    "# Extract ORB features\n",
    "def extract_orb_features(image, max_features=128):\n",
    "    orb = cv2.ORB_create()\n",
    "    _, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(max_features * 32)\n",
    "    if descriptors.shape[0] < max_features:\n",
    "        padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n",
    "        descriptors = np.vstack((descriptors, padding))\n",
    "    return descriptors[:max_features].flatten()\n",
    "\n",
    "# Extract histogram features (for grayscale images)\n",
    "def extract_histogram(image):\n",
    "    # Compute the histogram with 256 bins (grayscale)\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    # Normalize the histogram\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Combine selected features\n",
    "def extract_combined_features(image):\n",
    "    hog_features = extract_hog_features(image)\n",
    "    orb_features = extract_orb_features(image)\n",
    "    histogram = extract_histogram(image)\n",
    "    \n",
    "    return np.concatenate([hog_features, orb_features, histogram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(images[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(features, labels, output_csv):\n",
    "    # Combine features and labels\n",
    "    data = [np.append(label, feature) for feature, label in zip(features, labels)]\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False, header=False)\n",
    "    print(f\"Features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (28709, 48, 48)\n",
      "Test data shape: (7178, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(5252,)\n",
      "Features saved to train_features.csv\n",
      "Features saved to test_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract features for training and testing images\n",
    "X_train_features = [extract_combined_features(img) for img in X_train]\n",
    "X_test_features = [extract_combined_features(img) for img in X_test]\n",
    "\n",
    "# Step 2: Convert features to numpy arrays for scaling and PCA\n",
    "# X_train_features = np.array(train_features)\n",
    "# X_test_features = np.array(test_features)\n",
    "\n",
    "print(type(X_train_features))\n",
    "print(X_train_features[0].shape)  \n",
    "\n",
    "# Step 3: Standardize the training features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "\n",
    "# Step 4: Apply PCA to the scaled training features\n",
    "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Step 5: Save the processed training features to a CSV\n",
    "save_features_to_csv(X_train_pca, y_train, 'train_features.csv')\n",
    "\n",
    "# Step 6: Apply the same scaler and PCA to the test features\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 7: Save the processed testing features to a CSV\n",
    "save_features_to_csv(X_test_pca, y_test, 'test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 27/31 [09:51<02:22, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 28709, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -1.975426\n",
      "[LightGBM] [Info] Start training from score -4.191921\n",
      "[LightGBM] [Info] Start training from score -1.946712\n",
      "[LightGBM] [Info] Start training from score -1.386887\n",
      "[LightGBM] [Info] Start training from score -1.758429\n",
      "[LightGBM] [Info] Start training from score -1.777202\n",
      "[LightGBM] [Info] Start training from score -2.187829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [09:59<00:00, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
      "Model                                                                          \n",
      "SVC                                0.54               0.47    None      0.53   \n",
      "QuadraticDiscriminantAnalysis      0.48               0.45    None      0.48   \n",
      "LGBMClassifier                     0.49               0.44    None      0.48   \n",
      "RandomForestClassifier             0.48               0.42    None      0.46   \n",
      "ExtraTreesClassifier               0.48               0.41    None      0.46   \n",
      "KNeighborsClassifier               0.44               0.40    None      0.42   \n",
      "NearestCentroid                    0.39               0.39    None      0.40   \n",
      "LinearDiscriminantAnalysis         0.43               0.37    None      0.42   \n",
      "GaussianNB                         0.41               0.37    None      0.39   \n",
      "BaggingClassifier                  0.40               0.36    None      0.39   \n",
      "LogisticRegression                 0.43               0.36    None      0.41   \n",
      "CalibratedClassifierCV             0.43               0.35    None      0.41   \n",
      "LinearSVC                          0.42               0.34    None      0.39   \n",
      "LabelSpreading                     0.27               0.34    None      0.26   \n",
      "LabelPropagation                   0.27               0.34    None      0.26   \n",
      "RidgeClassifierCV                  0.42               0.33    None      0.38   \n",
      "RidgeClassifier                    0.42               0.33    None      0.38   \n",
      "DecisionTreeClassifier             0.34               0.32    None      0.34   \n",
      "BernoulliNB                        0.39               0.32    None      0.37   \n",
      "SGDClassifier                      0.38               0.32    None      0.35   \n",
      "AdaBoostClassifier                 0.37               0.29    None      0.34   \n",
      "ExtraTreeClassifier                0.29               0.29    None      0.29   \n",
      "Perceptron                         0.31               0.26    None      0.31   \n",
      "PassiveAggressiveClassifier        0.31               0.26    None      0.31   \n",
      "DummyClassifier                    0.25               0.14    None      0.10   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "SVC                                104.49  \n",
      "QuadraticDiscriminantAnalysis        0.45  \n",
      "LGBMClassifier                       8.01  \n",
      "RandomForestClassifier              41.50  \n",
      "ExtraTreesClassifier                 8.65  \n",
      "KNeighborsClassifier                 1.02  \n",
      "NearestCentroid                      0.15  \n",
      "LinearDiscriminantAnalysis           0.43  \n",
      "GaussianNB                           0.17  \n",
      "BaggingClassifier                   38.10  \n",
      "LogisticRegression                   0.36  \n",
      "CalibratedClassifierCV               4.86  \n",
      "LinearSVC                            1.27  \n",
      "LabelSpreading                     177.94  \n",
      "LabelPropagation                   170.92  \n",
      "RidgeClassifierCV                    0.36  \n",
      "RidgeClassifier                      0.20  \n",
      "DecisionTreeClassifier               6.12  \n",
      "BernoulliNB                          0.24  \n",
      "SGDClassifier                        3.76  \n",
      "AdaBoostClassifier                  28.77  \n",
      "ExtraTreeClassifier                  0.23  \n",
      "Perceptron                           0.58  \n",
      "PassiveAggressiveClassifier          0.85  \n",
      "DummyClassifier                      0.11  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "train_df = pd.read_csv('train_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.iloc[:, 1:].values  # Features\n",
    "y_train = train_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "test_df = pd.read_csv('test_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_test = test_df.iloc[:, 1:].values  # Features\n",
    "y_test = test_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Split the loaded data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use LazyClassifier for a quick comparison of models\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print out the model performance\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
