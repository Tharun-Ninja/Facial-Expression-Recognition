{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to process our augmented and oversampled dataset (Will combine ipynbs afterwards)\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define paths to the train and test directories\n",
    "train_dir = 'Dataset/Train'\n",
    "test_dir = 'Dataset/Test'\n",
    "\n",
    "# Check if the directories exist\n",
    "if not os.path.exists(train_dir):\n",
    "    raise FileNotFoundError(f\"Training directory {train_dir} not found.\")\n",
    "if not os.path.exists(test_dir):\n",
    "    raise FileNotFoundError(f\"Test directory {test_dir} not found.\")\n",
    "\n",
    "# Function to load images from a directory and its subdirectories\n",
    "def load_images_from_directory(directory):\n",
    "    images = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        class_name = os.path.basename(root)\n",
    "        images[class_name] = []\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(root, filename)\n",
    "                with Image.open(img_path) as img:\n",
    "                    img_array = np.array(img)\n",
    "                    images[class_name].append(img_array)\n",
    "    return images\n",
    "\n",
    "# Load images from the train and test directories\n",
    "train_images = load_images_from_directory(train_dir)\n",
    "test_images = load_images_from_directory(test_dir)\n",
    "\n",
    "# Print the number of images loaded for each class\n",
    "for class_name, images in train_images.items():\n",
    "    print(f\"Loaded {len(images)} images from class {class_name} in {train_dir}\")\n",
    "for class_name, images in test_images.items():\n",
    "    print(f\"Loaded {len(images)} images from class {class_name} in {test_dir}\")\n",
    "\n",
    "if \"Train\" in train_images:train_images.pop(\"Train\")\n",
    "if \"Test\" in test_images:test_images.pop(\"Test\")\n",
    "\n",
    "print(train_images.keys())\n",
    "print(test_images.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def augment_image(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "\n",
    "    # Random rotation\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.uniform(-30, 30)\n",
    "        pil_image = pil_image.rotate(angle)\n",
    "\n",
    "    # Random horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        pil_image = ImageOps.mirror(pil_image)\n",
    "\n",
    "    # Random vertical flip\n",
    "    if random.random() > 0.5:\n",
    "        pil_image = ImageOps.flip(pil_image)\n",
    "\n",
    "    # Random Gaussian noise\n",
    "    if random.random() > 0.5:\n",
    "        np_image = np.array(pil_image)\n",
    "        mean = 0\n",
    "        std = random.uniform(0, 25)\n",
    "        gauss = np.random.normal(mean, std, np_image.shape).astype('uint8')\n",
    "        np_image = np.clip(np_image + gauss, 0, 255)\n",
    "        pil_image = Image.fromarray(np_image)\n",
    "\n",
    "    # Convert PIL Image back to numpy array\n",
    "    return np.array(pil_image)\n",
    "\n",
    "# Apply augmentation to all images in the train_images dictionary\n",
    "augmented_train_images = {}\n",
    "for class_name, images in train_images.items():\n",
    "    augmented_train_images[class_name] = [augment_image(image) for image in images]\n",
    "\n",
    "# Print the number of augmented images for each class\n",
    "for class_name, images in augmented_train_images.items():\n",
    "    print(f\"Augmented {len(images)} images for class {class_name}\")\n",
    "\n",
    "# Merge augmented images with original training images\n",
    "for class_name, images in augmented_train_images.items():\n",
    "    if class_name in train_images:\n",
    "        train_images[class_name].extend(images)\n",
    "    else:\n",
    "        train_images[class_name] = images\n",
    "\n",
    "# Count the number of images in each class for train and test datasets\n",
    "train_counts = {class_name: len(images) for class_name, images in train_images.items()}\n",
    "test_counts = {class_name: len(images) for class_name, images in test_images.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Calculate the target number of images for each class (equal to the maximum class size)\n",
    "max_class_size = max(train_counts.values())\n",
    "print(f\"Maximum class size: {max_class_size}\")\n",
    "\n",
    "# Step 2: Function to oversample a class by augmenting images\n",
    "def oversample_class(images, target_size):\n",
    "    augmented_images = []\n",
    "    while len(images) + len(augmented_images) < target_size:\n",
    "        # Augment the images to reach the target size\n",
    "        image = random.choice(images)  # Randomly pick an image from the class\n",
    "        augmented_image = augment_image(image)  # Augment the selected image\n",
    "        augmented_images.append(augmented_image)\n",
    "    return images + augmented_images\n",
    "\n",
    "# Step 3: Oversample each class in the train_images dictionary\n",
    "for class_name, images in train_images.items():\n",
    "    if len(images) < max_class_size:\n",
    "        # If the class has fewer images than the max_class_size, oversample it\n",
    "        train_images[class_name] = oversample_class(images, max_class_size)\n",
    "    print(f\"Oversampled class {class_name} to {len(train_images[class_name])} images\")\n",
    "\n",
    "# Step 4: Recalculate the number of images in each class after oversampling\n",
    "train_counts = {class_name: len(images) for class_name, images in train_images.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test = [],[],[],[]\n",
    "\n",
    "for label, images in train_images.items():\n",
    "    for img in images:\n",
    "        X_train.append(img.flatten())  \n",
    "        y_train.append(label)\n",
    "    \n",
    "for label, images in test_images.items():\n",
    "    for img in images:\n",
    "        X_test.append(img.flatten())\n",
    "        y_test.append(label)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust']\n",
      "(35887, 48, 48) (35887,)\n"
     ]
    }
   ],
   "source": [
    "# # Loading the Dataset\n",
    "# train_dir = 'Dataset/Train'\n",
    "# test_dir = 'Dataset/Test'\n",
    "\n",
    "# print(os.listdir(train_dir))\n",
    "\n",
    "# def get_data(dir):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for cat in os.listdir(dir):\n",
    "#         for img_name in os.listdir(os.path.join(dir, cat)):\n",
    "#             img = cv2.imread(os.path.join(dir,cat,img_name), cv2.IMREAD_GRAYSCALE)\n",
    "#             images.append(img)\n",
    "#             labels.append(cat)\n",
    "    \n",
    "#     return np.array(images), np.array(labels)\n",
    "\n",
    "# train_images, train_labels = get_data(train_dir)\n",
    "# test_images, test_labels = get_data(test_dir)\n",
    "\n",
    "# all_images = np.concatenate((train_images, test_images))\n",
    "# all_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "# print(all_images.shape, all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
      "happy       8989\n",
      "neutral     6198\n",
      "sad         6077\n",
      "fear        5121\n",
      "angry       4953\n",
      "surprise    4002\n",
      "disgust      547\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(np.unique(all_labels))\n",
    "# # describe the labels\n",
    "# print(pd.Series(all_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert flattened images into a list of strings for saving\n",
    "# image_strings = [' '.join(map(str, img)) for img in all_images]\n",
    "\n",
    "# # Create a DataFrame with image data as a single column and labels\n",
    "# df = pd.DataFrame({'image_data': image_strings, 'label': all_labels})\n",
    "\n",
    "# # Save to CSV\n",
    "# df.to_csv('all_images_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to all_images_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# # Saving the images and labels to a CSV file\n",
    "# def save_images_labels_to_csv(images, labels, output_csv):\n",
    "#     with open(output_csv, 'w') as f:\n",
    "#         # Write the header for image pixels and the label\n",
    "#         columns = ['label'] + [f'pixel_{i}' for i in range(images[0].size)] \n",
    "#         f.write(','.join(columns) + '\\n')\n",
    "        \n",
    "#         # Process and write each image's flattened data\n",
    "#         for image, label in zip(images, labels):\n",
    "#             # Flatten the image to a 1D array\n",
    "#             flattened_image = image.flatten()\n",
    "#             # Convert the array to a string format suitable for CSV\n",
    "#             row = np.append(label, flattened_image)\n",
    "#             row_str = ','.join(map(str, row))\n",
    "#             # Write the row to the CSV\n",
    "#             f.write(row_str + '\\n')\n",
    "    \n",
    "#     print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "# # Save all images and labels to a single CSV file\n",
    "# save_images_labels_to_csv(all_images, all_labels, 'all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Train data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the images from the csv\n",
    "# image_size = 48  # Assuming 48x48 images\n",
    "\n",
    "# # Load the data from the CSV\n",
    "# def load_images_labels_from_csv(csv_file):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     labels = df['label'].values\n",
    "#     # Drop the label column to retain only pixel data\n",
    "#     image_data = df.drop(columns=['label']).values\n",
    "#     # Reshape the image data back to original shape (48x48)\n",
    "#     images = [np.array(image, dtype=np.uint8).reshape((image_size, image_size)) for image in image_data]\n",
    "#     return images, labels\n",
    "\n",
    "# # Load images and labels\n",
    "# all_images, all_labels = load_images_labels_from_csv('all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887\n"
     ]
    }
   ],
   "source": [
    "# print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    return hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "\n",
    "# Extract ORB features\n",
    "def extract_orb_features(image, max_features=128):\n",
    "    orb = cv2.ORB_create()\n",
    "    _, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(max_features * 32)\n",
    "    if descriptors.shape[0] < max_features:\n",
    "        padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n",
    "        descriptors = np.vstack((descriptors, padding))\n",
    "    return descriptors[:max_features].flatten()\n",
    "\n",
    "# Extract histogram features (for grayscale images)\n",
    "def extract_histogram(image):\n",
    "    # Compute the histogram with 256 bins (grayscale)\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    # Normalize the histogram\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Combine selected features\n",
    "def extract_combined_features(image):\n",
    "    hog_features = extract_hog_features(image)\n",
    "    orb_features = extract_orb_features(image)\n",
    "    histogram = extract_histogram(image)\n",
    "    \n",
    "    return np.concatenate([hog_features, orb_features, histogram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(all_images[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(features, labels, output_csv):\n",
    "    # Combine features and labels\n",
    "    data = [np.append(label, feature) for feature, label in zip(features, labels)]\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False, header=False)\n",
    "    print(f\"Features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Train data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(5252,)\n",
      "Features saved to train_features.csv\n",
      "Features saved to test_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract features for training and testing images\n",
    "X_train_features = [extract_combined_features(img) for img in X_train]\n",
    "X_test_features = [extract_combined_features(img) for img in X_test]\n",
    "\n",
    "# Step 2: Convert features to numpy arrays for scaling and PCA\n",
    "# X_train_features = np.array(train_features)\n",
    "# X_test_features = np.array(test_features)\n",
    "\n",
    "print(type(X_train_features))\n",
    "print(X_train_features[0].shape)  \n",
    "\n",
    "# Step 3: Standardize the training features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "\n",
    "# Step 4: Apply PCA to the scaled training features\n",
    "pca = PCA(n_components=200)  # Adjust n_components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Step 5: Save the processed training features to a CSV\n",
    "save_features_to_csv(X_train_pca, y_train, 'train_features.csv')\n",
    "\n",
    "# Step 6: Apply the same scaler and PCA to the test features\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 7: Save the processed testing features to a CSV\n",
    "save_features_to_csv(X_test_pca, y_test, 'test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [14:02<00:34, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 28709, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.975426\n",
      "[LightGBM] [Info] Start training from score -4.191921\n",
      "[LightGBM] [Info] Start training from score -1.946712\n",
      "[LightGBM] [Info] Start training from score -1.386887\n",
      "[LightGBM] [Info] Start training from score -1.758429\n",
      "[LightGBM] [Info] Start training from score -1.777202\n",
      "[LightGBM] [Info] Start training from score -2.187829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [14:18<00:00, 27.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
      "Model                                                                          \n",
      "SVC                                0.52               0.45    None      0.51   \n",
      "QuadraticDiscriminantAnalysis      0.49               0.45    None      0.49   \n",
      "LGBMClassifier                     0.48               0.43    None      0.47   \n",
      "NearestCentroid                    0.40               0.40    None      0.41   \n",
      "RandomForestClassifier             0.45               0.39    None      0.43   \n",
      "LogisticRegression                 0.44               0.38    None      0.42   \n",
      "LinearDiscriminantAnalysis         0.43               0.38    None      0.42   \n",
      "KNeighborsClassifier               0.41               0.37    None      0.39   \n",
      "ExtraTreesClassifier               0.44               0.37    None      0.41   \n",
      "CalibratedClassifierCV             0.44               0.36    None      0.41   \n",
      "GaussianNB                         0.40               0.36    None      0.38   \n",
      "BaggingClassifier                  0.38               0.35    None      0.38   \n",
      "LinearSVC                          0.43               0.35    None      0.40   \n",
      "SGDClassifier                      0.39               0.34    None      0.38   \n",
      "RidgeClassifier                    0.42               0.34    None      0.39   \n",
      "RidgeClassifierCV                  0.42               0.33    None      0.39   \n",
      "BernoulliNB                        0.38               0.32    None      0.37   \n",
      "DecisionTreeClassifier             0.33               0.32    None      0.33   \n",
      "AdaBoostClassifier                 0.36               0.29    None      0.33   \n",
      "Perceptron                         0.32               0.28    None      0.32   \n",
      "ExtraTreeClassifier                0.28               0.28    None      0.28   \n",
      "PassiveAggressiveClassifier        0.32               0.27    None      0.32   \n",
      "LabelSpreading                     0.21               0.25    None      0.16   \n",
      "LabelPropagation                   0.21               0.25    None      0.16   \n",
      "DummyClassifier                    0.25               0.14    None      0.10   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "SVC                                232.59  \n",
      "QuadraticDiscriminantAnalysis        0.65  \n",
      "LGBMClassifier                      15.32  \n",
      "NearestCentroid                      0.23  \n",
      "RandomForestClassifier              57.85  \n",
      "LogisticRegression                   0.56  \n",
      "LinearDiscriminantAnalysis           0.74  \n",
      "KNeighborsClassifier                 1.31  \n",
      "ExtraTreesClassifier                12.50  \n",
      "CalibratedClassifierCV               9.20  \n",
      "GaussianNB                           0.27  \n",
      "BaggingClassifier                   77.43  \n",
      "LinearSVC                            2.21  \n",
      "SGDClassifier                       13.38  \n",
      "RidgeClassifier                      0.28  \n",
      "RidgeClassifierCV                    0.61  \n",
      "BernoulliNB                          0.35  \n",
      "DecisionTreeClassifier              11.70  \n",
      "AdaBoostClassifier                  55.19  \n",
      "Perceptron                           0.81  \n",
      "ExtraTreeClassifier                  0.32  \n",
      "PassiveAggressiveClassifier          1.65  \n",
      "LabelSpreading                     189.54  \n",
      "LabelPropagation                   172.82  \n",
      "DummyClassifier                      0.16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "train_df = pd.read_csv('train_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.iloc[:, 1:].values  # Features\n",
    "y_train = train_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "test_df = pd.read_csv('test_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_test = test_df.iloc[:, 1:].values  # Features\n",
    "y_test = test_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Split the loaded data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use LazyClassifier for a quick comparison of models\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print out the model performance\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 392    0   71  154  139  184   31]\n",
      " [  27   22   10   14    5   28    7]\n",
      " [ 115    1  335  151  125  201   95]\n",
      " [  72    0   68 1395  113  140   28]\n",
      " [  85    0   84  203  644  212   23]\n",
      " [ 123    0  105  205  189  581   19]\n",
      " [  42    0   65   78   65   43  489]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.46      0.40      0.43       971\n",
      "     disgust       0.96      0.19      0.32       113\n",
      "        fear       0.45      0.33      0.38      1023\n",
      "       happy       0.63      0.77      0.69      1816\n",
      "     neutral       0.50      0.51      0.51      1251\n",
      "         sad       0.42      0.48      0.45      1222\n",
      "    surprise       0.71      0.63      0.66       782\n",
      "\n",
      "    accuracy                           0.54      7178\n",
      "   macro avg       0.59      0.47      0.49      7178\n",
      "weighted avg       0.54      0.54      0.53      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed features and labels from CSV\n",
    "train_df = pd.read_csv('train_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.iloc[:, 1:].values  # Features\n",
    "y_train = train_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "test_df = pd.read_csv('test_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_test = test_df.iloc[:, 1:].values  # Features\n",
    "y_test = test_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVC model\n",
    "model = SVC(kernel='rbc')  # You can change the kernel to 'rbf', 'poly', etc.\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy:\n",
      "0.5374756199498467\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Accuracy:\")\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
