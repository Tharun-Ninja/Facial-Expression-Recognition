{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 images from class Train in Dataset/Train\n",
      "Loaded 7215 images from class happy in Dataset/Train\n",
      "Loaded 4830 images from class sad in Dataset/Train\n",
      "Loaded 4097 images from class fear in Dataset/Train\n",
      "Loaded 3171 images from class surprise in Dataset/Train\n",
      "Loaded 4965 images from class neutral in Dataset/Train\n",
      "Loaded 3995 images from class angry in Dataset/Train\n",
      "Loaded 436 images from class disgust in Dataset/Train\n",
      "Loaded 0 images from class Test in Dataset/Test\n",
      "Loaded 1774 images from class happy in Dataset/Test\n",
      "Loaded 1247 images from class sad in Dataset/Test\n",
      "Loaded 1024 images from class fear in Dataset/Test\n",
      "Loaded 831 images from class surprise in Dataset/Test\n",
      "Loaded 1233 images from class neutral in Dataset/Test\n",
      "Loaded 958 images from class angry in Dataset/Test\n",
      "Loaded 111 images from class disgust in Dataset/Test\n",
      "dict_keys(['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust'])\n",
      "dict_keys(['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust'])\n",
      "Augmented 7215 images for class happy\n",
      "Augmented 4830 images for class sad\n",
      "Augmented 4097 images for class fear\n",
      "Augmented 3171 images for class surprise\n",
      "Augmented 4965 images for class neutral\n",
      "Augmented 3995 images for class angry\n",
      "Augmented 436 images for class disgust\n",
      "Maximum class size: 14430\n",
      "Oversampled class happy to 14430 images\n",
      "Oversampled class sad to 14430 images\n",
      "Oversampled class fear to 14430 images\n",
      "Oversampled class surprise to 14430 images\n",
      "Oversampled class neutral to 14430 images\n",
      "Oversampled class angry to 14430 images\n",
      "Oversampled class disgust to 14430 images\n"
     ]
    }
   ],
   "source": [
    "# Needed to process our augmented and oversampled dataset (Will combine ipynbs afterwards)\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define paths to the train and test directories\n",
    "train_dir = 'Dataset/Train'\n",
    "test_dir = 'Dataset/Test'\n",
    "\n",
    "# Check if the directories exist\n",
    "if not os.path.exists(train_dir):\n",
    "    raise FileNotFoundError(f\"Training directory {train_dir} not found.\")\n",
    "if not os.path.exists(test_dir):\n",
    "    raise FileNotFoundError(f\"Test directory {test_dir} not found.\")\n",
    "\n",
    "# Function to load images from a directory and its subdirectories\n",
    "def load_images_from_directory(directory):\n",
    "    images = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        class_name = os.path.basename(root)\n",
    "        images[class_name] = []\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(root, filename)\n",
    "                with Image.open(img_path) as img:\n",
    "                    img_array = np.array(img)\n",
    "                    images[class_name].append(img_array)\n",
    "    return images\n",
    "\n",
    "# Load images from the train and test directories\n",
    "train_images = load_images_from_directory(train_dir)\n",
    "test_images = load_images_from_directory(test_dir)\n",
    "\n",
    "# Print the number of images loaded for each class\n",
    "for class_name, images in train_images.items():\n",
    "    print(f\"Loaded {len(images)} images from class {class_name} in {train_dir}\")\n",
    "for class_name, images in test_images.items():\n",
    "    print(f\"Loaded {len(images)} images from class {class_name} in {test_dir}\")\n",
    "\n",
    "if \"Train\" in train_images:train_images.pop(\"Train\")\n",
    "if \"Test\" in test_images:test_images.pop(\"Test\")\n",
    "\n",
    "print(train_images.keys())\n",
    "print(test_images.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def augment_image(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "\n",
    "    # Random rotation\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.uniform(-30, 30)\n",
    "        pil_image = pil_image.rotate(angle)\n",
    "\n",
    "    # Random horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        pil_image = ImageOps.mirror(pil_image)\n",
    "\n",
    "    # Random vertical flip\n",
    "    if random.random() > 0.5:\n",
    "        pil_image = ImageOps.flip(pil_image)\n",
    "\n",
    "    # Random Gaussian noise\n",
    "    if random.random() > 0.5:\n",
    "        np_image = np.array(pil_image)\n",
    "        mean = 0\n",
    "        std = random.uniform(0, 25)\n",
    "        gauss = np.random.normal(mean, std, np_image.shape).astype('uint8')\n",
    "        np_image = np.clip(np_image + gauss, 0, 255)\n",
    "        pil_image = Image.fromarray(np_image)\n",
    "\n",
    "    # Convert PIL Image back to numpy array\n",
    "    return np.array(pil_image)\n",
    "\n",
    "# Apply augmentation to all images in the train_images dictionary\n",
    "augmented_train_images = {}\n",
    "for class_name, images in train_images.items():\n",
    "    augmented_train_images[class_name] = [augment_image(image) for image in images]\n",
    "\n",
    "# Print the number of augmented images for each class\n",
    "for class_name, images in augmented_train_images.items():\n",
    "    print(f\"Augmented {len(images)} images for class {class_name}\")\n",
    "\n",
    "# Merge augmented images with original training images\n",
    "for class_name, images in augmented_train_images.items():\n",
    "    if class_name in train_images:\n",
    "        train_images[class_name].extend(images)\n",
    "    else:\n",
    "        train_images[class_name] = images\n",
    "\n",
    "# Count the number of images in each class for train and test datasets\n",
    "train_counts = {class_name: len(images) for class_name, images in train_images.items()}\n",
    "test_counts = {class_name: len(images) for class_name, images in test_images.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Calculate the target number of images for each class (equal to the maximum class size)\n",
    "max_class_size = max(train_counts.values())\n",
    "print(f\"Maximum class size: {max_class_size}\")\n",
    "\n",
    "# Step 2: Function to oversample a class by augmenting images\n",
    "def oversample_class(images, target_size):\n",
    "    augmented_images = []\n",
    "    while len(images) + len(augmented_images) < target_size:\n",
    "        # Augment the images to reach the target size\n",
    "        image = random.choice(images)  # Randomly pick an image from the class\n",
    "        augmented_image = augment_image(image)  # Augment the selected image\n",
    "        augmented_images.append(augmented_image)\n",
    "    return images + augmented_images\n",
    "\n",
    "# Step 3: Oversample each class in the train_images dictionary\n",
    "for class_name, images in train_images.items():\n",
    "    if len(images) < max_class_size:\n",
    "        # If the class has fewer images than the max_class_size, oversample it\n",
    "        train_images[class_name] = oversample_class(images, max_class_size)\n",
    "    print(f\"Oversampled class {class_name} to {len(train_images[class_name])} images\")\n",
    "\n",
    "# Step 4: Recalculate the number of images in each class after oversampling\n",
    "train_counts = {class_name: len(images) for class_name, images in train_images.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "# Flatten training images and store them along with their labels\n",
    "for label, images in train_images.items():\n",
    "    for img in images:\n",
    "        X_train.append(img)  \n",
    "        y_train.append(label)\n",
    "\n",
    "# Flatten testing images and store them along with their labels\n",
    "for label, images in test_images.items():\n",
    "    for img in images:\n",
    "        X_test.append(img)\n",
    "        y_test.append(label)\n",
    "\n",
    "# Convert the lists to numpy arrays after collecting all data\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the Dataset\n",
    "# train_dir = 'Dataset/Train'\n",
    "# test_dir = 'Dataset/Test'\n",
    "\n",
    "# print(os.listdir(train_dir))\n",
    "\n",
    "# def get_data(dir):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for cat in os.listdir(dir):\n",
    "#         for img_name in os.listdir(os.path.join(dir, cat)):\n",
    "#             img = cv2.imread(os.path.join(dir,cat,img_name), cv2.IMREAD_GRAYSCALE)\n",
    "#             images.append(img)\n",
    "#             labels.append(cat)\n",
    "    \n",
    "#     return np.array(images), np.array(labels)\n",
    "\n",
    "# train_images, train_labels = get_data(train_dir)\n",
    "# test_images, test_labels = get_data(test_dir)\n",
    "\n",
    "# all_images = np.concatenate((train_images, test_images))\n",
    "# all_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "# print(all_images.shape, all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(all_labels))\n",
    "# # describe the labels\n",
    "# print(pd.Series(all_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert flattened images into a list of strings for saving\n",
    "# image_strings = [' '.join(map(str, img)) for img in all_images]\n",
    "\n",
    "# # Create a DataFrame with image data as a single column and labels\n",
    "# df = pd.DataFrame({'image_data': image_strings, 'label': all_labels})\n",
    "\n",
    "# # Save to CSV\n",
    "# df.to_csv('all_images_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the images and labels to a CSV file\n",
    "# def save_images_labels_to_csv(images, labels, output_csv):\n",
    "#     with open(output_csv, 'w') as f:\n",
    "#         # Write the header for image pixels and the label\n",
    "#         columns = ['label'] + [f'pixel_{i}' for i in range(images[0].size)] \n",
    "#         f.write(','.join(columns) + '\\n')\n",
    "        \n",
    "#         # Process and write each image's flattened data\n",
    "#         for image, label in zip(images, labels):\n",
    "#             # Flatten the image to a 1D array\n",
    "#             flattened_image = image.flatten()\n",
    "#             # Convert the array to a string format suitable for CSV\n",
    "#             row = np.append(label, flattened_image)\n",
    "#             row_str = ','.join(map(str, row))\n",
    "#             # Write the row to the CSV\n",
    "#             f.write(row_str + '\\n')\n",
    "    \n",
    "#     print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "# # Save all images and labels to a single CSV file\n",
    "# save_images_labels_to_csv(all_images, all_labels, 'all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Train data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the images from the csv\n",
    "# image_size = 48  # Assuming 48x48 images\n",
    "\n",
    "# # Load the data from the CSV\n",
    "# def load_images_labels_from_csv(csv_file):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     labels = df['label'].values\n",
    "#     # Drop the label column to retain only pixel data\n",
    "#     image_data = df.drop(columns=['label']).values\n",
    "#     # Reshape the image data back to original shape (48x48)\n",
    "#     images = [np.array(image, dtype=np.uint8).reshape((image_size, image_size)) for image in image_data]\n",
    "#     return images, labels\n",
    "\n",
    "# # Load images and labels\n",
    "# all_images, all_labels = load_images_labels_from_csv('all_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    return hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "\n",
    "# Extract ORB features\n",
    "def extract_orb_features(image, max_features=128):\n",
    "    orb = cv2.ORB_create()\n",
    "    _, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(max_features * 32)\n",
    "    if descriptors.shape[0] < max_features:\n",
    "        padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n",
    "        descriptors = np.vstack((descriptors, padding))\n",
    "    return descriptors[:max_features].flatten()\n",
    "\n",
    "# Extract histogram features (for grayscale images)\n",
    "def extract_histogram(image):\n",
    "    # Compute the histogram with 256 bins (grayscale)\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    # Normalize the histogram\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Combine selected features\n",
    "def extract_combined_features(image):\n",
    "    hog_features = extract_hog_features(image)\n",
    "    orb_features = extract_orb_features(image)\n",
    "    histogram = extract_histogram(image)\n",
    "    \n",
    "    return np.concatenate([hog_features, orb_features, histogram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(all_images[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(features, labels, output_csv):\n",
    "    # Combine features and labels\n",
    "    data = [np.append(label, feature) for feature, label in zip(features, labels)]\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False, header=False)\n",
    "    print(f\"Features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Train data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101010, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(5252,)\n",
      "Features saved to train_features.csv\n",
      "Features saved to test_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract features for training and testing images\n",
    "X_train_features = [extract_combined_features(img) for img in X_train]\n",
    "X_test_features = [extract_combined_features(img) for img in X_test]\n",
    "\n",
    "# Step 2: Convert features to numpy arrays for scaling and PCA\n",
    "# X_train_features = np.array(train_features)\n",
    "# X_test_features = np.array(test_features)\n",
    "\n",
    "print(type(X_train_features))\n",
    "print(X_train_features[0].shape)  \n",
    "\n",
    "# Step 3: Standardize the training features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "\n",
    "# Step 4: Apply PCA to the scaled training features\n",
    "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Step 5: Save the processed training features to a CSV\n",
    "save_features_to_csv(X_train_pca, y_train, 'train_features.csv')\n",
    "\n",
    "# Step 6: Apply the same scaler and PCA to the test features\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 7: Save the processed testing features to a CSV\n",
    "save_features_to_csv(X_test_pca, y_test, 'test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from lazypredict.Supervised import LazyClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the preprocessed features and labels from CSV\n",
    "# train_df = pd.read_csv('train_features.csv', header=None)\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_train = train_df.iloc[:, 1:].values  # Features\n",
    "# y_train = train_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# # Load the preprocessed features and labels from CSV\n",
    "# test_df = pd.read_csv('test_features.csv', header=None)\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_test = test_df.iloc[:, 1:].values  # Features\n",
    "# y_test = test_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# # Split the loaded data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Use LazyClassifier for a quick comparison of models\n",
    "# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "# models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# # Print out the model performance\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 405   23   78  113  150  146   43]\n",
      " [  19   64    7   10    1    8    2]\n",
      " [ 128   11  359   94  134  196  102]\n",
      " [  83    5   62 1351  116  134   23]\n",
      " [  99    9   87  156  652  183   47]\n",
      " [ 157   11  111  153  211  578   26]\n",
      " [  33    8   64   53   64   43  566]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.44      0.42      0.43       958\n",
      "     disgust       0.49      0.58      0.53       111\n",
      "        fear       0.47      0.35      0.40      1024\n",
      "       happy       0.70      0.76      0.73      1774\n",
      "     neutral       0.49      0.53      0.51      1233\n",
      "         sad       0.45      0.46      0.46      1247\n",
      "    surprise       0.70      0.68      0.69       831\n",
      "\n",
      "    accuracy                           0.55      7178\n",
      "   macro avg       0.53      0.54      0.53      7178\n",
      "weighted avg       0.55      0.55      0.55      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed features and labels from CSV\n",
    "train_df = pd.read_csv('train_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.iloc[:, 1:].values  # Features\n",
    "y_train = train_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Load the preprocessed features and labels from CSV\n",
    "test_df = pd.read_csv('test_features.csv', header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "X_test = test_df.iloc[:, 1:].values  # Features\n",
    "y_test = test_df.iloc[:, 0].values  # Labels (assuming label is in the first column)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVC model\n",
    "model = SVC(kernel='rbf')  # You can change the kernel to 'rbf', 'poly', etc.\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy:\n",
      "0.5537754249094455\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Accuracy:\")\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc_model2.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
